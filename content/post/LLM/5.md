---
title: LLM Evaluations (1/?)
date: 2025-10-31
categories:
- LLM
thumbnailImage: /images/LLM/5/2.webp
summary: LLM 평가 방법에 대한 전반적인 소개
katex: true
---
## LLM Product
LLM, RAG, MCP와 같은 기술을 기반으로 한 LLM 제품은 이제 다양한 형태로 우리 곁에 자리 잡고 있습니다. 고객과 직접 소통하는 <strong>챗봇(User-facing features)</strong>부터 마케팅 카피를 만드는 <strong>내부 도구(Internal tools)</strong>까지 그 종류도 다양합니다.

기존 소프트웨어에 LLM을 더해 완전히 새로운 경험을 제공하기도 합니다. 예를 들어, 자연어 쿼리만으로 데이터를 검색(NL2SQL)하게 만들거나, 회의실 예약이나 휴가 신청 같은 일상적인 업무를 대신 처리해 주는 비서 애플리케이션을 개발하기도 합니다.

실제 기업들의 활용 사례를 통해 알아보자면:
- {{<hl-text primary>}}Vimeo{{</hl-text>}}는 LLM 기반의 <strong>[고객센터 챗봇](https://medium.com/vimeo-engineering-blog/from-idea-to-reality-elevating-our-customer-support-through-generative-ai-101a2c5ea680)</strong>을 구축하여 고객 문의에 대응하고 있습니다.
- {{<hl-text primary>}}Pinterest{{</hl-text>}} 데이터 분석가이 <strong>[자연어로 SQL을 작성](https://medium.com/pinterest-engineering/how-we-built-text-to-sql-at-pinterest-30bad30dabff)</strong>할 수 있는 LLM 시스템을 개발했습니다.
- {{<hl-text primary>}}Yelp{{</hl-text>}}는 LLM을 활용해 플랫폼의 <strong>[부적절한 리뷰를 자동으로 탐지](https://engineeringblog.yelp.com/2024/03/ai-pipeline-inappropriate-language-detection.html)</strong>합니다.
- {{<hl-text primary>}}Amazon{{</hl-text>}}은 지식 그래프와 LLM을 결합하여 고객의 질문 의도에 가장 적합한 <strong>[제품을 추천](https://www.amazon.science/blog/building-commonsense-knowledge-graphs-to-aid-product-recommendation)</strong>합니다.
- {{<hl-text primary>}}OLX{{</hl-text>}}는 LLM을 활용해 <strong>[구인 광고에서 핵심 직무 역량을 식별](https://tech.olx.com/extracting-job-roles-in-job-ads-a-journey-with-generative-ai-e8b8cf399659)</strong>하여 구직자와 기업을 더 정확히 연결합니다.

> 더 많은 사례는 [Evidently AI Blog](https://www.evidentlyai.com/blog/llm-applications)에서 확인하실 수 있습니다.

LLM을 활용한 제품들이 시장에 쏟아져 나오면서, 기술 기업들의 홍보 경쟁도 그만큼 치열해지고 있습니다. 이러한 경쟁에서 소비자의 눈길을 사로잡는 가장 확실한 방법은 '타사 대비 N% 성능 향상'과 같이 명확한 <strong>정량적 지표</strong>를 제시하는 것입니다.

그런 측면에서 가장 흔하게 사용하는 지표는 <strong>속도</strong>와 <strong>가격</strong>입니다. 가격은 제품 선택에 있어 중요한 기준이며, 속도 또한 무시할 수 없는 요소입니다. 하지만 정작 가장 중요한 <strong>성능</strong>에 대한 구체적인 지표는 찾아보기 어렵습니다.

물론, 가끔씩 의도적으로 성능 지표를 감추는 경우도 있습니다. 하지만 현재 시장에서 성능 지표가 잘 보이지 않는 이유는 '의도'라기보다 '어쩔 수 없는' 현실에 가깝습니다. {{<hl-text primary>}}왜냐하면 LLM '제품은 만드는 것'보다, 그 제품을 제대로 '평가하는 방법을 만드는 것'이 훨씬 더 어렵기 때문입니다.{{</hl-text>}}

---
## LLM Evaluation
LLM 평가는 크게 두 가지 맥락으로 나눌 수 있습니다.

- LLM 자체의 성능 평가
- LLM 제품의 성능 평가

이 두 평가는 일부 방법을 공유할 수 있지만 그 관점이 근본적으로 다릅니다. 따라서 평가 방법을 구축할 때 모델 자체에 대한 것인지, 아니면 최종 제품에 대한 것인지를 구분하는 것은 매우 중요합니다.

### LLM 모델 평가
LLM 모델 자체를 평가한다는 것은 코딩, 번역, 수학 문제 풀이처럼 모델이 가진 <strong>순수한(raw) 능력</strong>에 집중하는 것을 의미합니다. 연구자들은 이를 위해 표준화된 LLM 벤치마크를 사용합니다.

예를 들어, 벤치마크는 다음과 같은 능력을 측정할 수 있습니다.
- 모델이 역사적 사실을 얼마나 정확하게 알고 있는가? (지식)
- 주어진 정보를 바탕으로 얼마나 논리적인 추론을 하는가? (추론 능력)
- 안전하지 않거나 악의적인 입력에 어떻게 반응하는가? (안전성)

현재 세상에는 수백 개의 LLM 벤치마크가 존재하며, 각 벤치마크는 고유한 테스트 데이터셋을 가집니다. 대부분은 정답이 정해진 질문들을 통해 모델의 답변과 정답을 비교하는 방식으로 점수를 매깁니다. 일부는 크라우드소싱으로 답변의 품질 순위를 매기는 등 더 복잡한 방식을 사용하기도 합니다.

{{<image classes="fig-100 center" src="/images/LLM/5/1.webp">}}
> 그림 1. Example questions from the MMLU benchmark<br>
[MEASURING MASSIVE MULTITASK LANGUAGE UNDERSTANDING](https://arxiv.org/abs/2009.03300)

이러한 LLM 벤치마크 덕분에 우리는 여러 모델의 성능을 객관적으로 비교할 수 있습니다. Hugging Face의 [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/)처럼 공개된 리더보드는 "코딩을 가장 잘하는 오픈소스 LLM은 무엇일까?&rdquo;와 같은 질문에 대한 답을 찾는 데 큰 도움을 줍니다.

하지만 중요한 점이 있습니다. LLM 벤치마크는 새로운 모델을 선택하거나 기술 발전 동향을 파악하는 데는 매우 유용하지만, 우리가 만드는 실제 제품의 성능을 평가하는 데는 적합하지 않습니다.

벤치마크는 특정 제품이 다룰 좁은 범위의 입력이 아닌, 모델의 전반적이고 광범위한 능력을 테스트하기 때문입니다. 또한, 벤치마크는 오직 LLM의 단독 성능에만 초점을 맞춥니다. 반면, 실제 제품은 검색(RAG), 외부 도구 연동(MCP) 등 LLM 외에도 수많은 구성 요소가 결합된 복잡한 시스템이기 때문입니다.

### LLM 제품 평가
반면, LLM 제품 평가는 <strong>특정 목표</strong>를 해결하기 위해 구축된 <strong>전체 시스템</strong>의 성능을 측정합니다.

여기서 평가 대상은 LLM 하나에 국한되지 않습니다. 정교하게 설계된 프롬프트, 여러 프롬프트를 연결하는 비즈니스 로직, 답변의 정확도를 높이기 위해 참조하는 외부 데이터베이스(RAG) 등 제품을 구성하는 모든 요소가 평가에 포함됩니다.

또한 평가는 '실제 고객 지원 문의'나 '내부 데이터 분석 요청'처럼 해당 제품이 실제로 마주할 현실적인 데이터를 기반으로 이루어집니다. 범용적인 능력이 아닌, 특정 사용 사례(Use Case)에서 얼마나 제 역할을 하는지를 검증하는 것이 핵심입니다.

{{<image classes="fig-100 center" src="/images/LLM/5/2.webp">}}
> 그림 2. LLM 제품 평가의 상대적 어려움<br>
[Evidently AI](https://www.evidentlyai.com/llm-guide/llm-evaluation)

이러한 LLM 애플리케이션을 평가할 때는 일반적으로 두 가지 큰 축을 중심으로 접근합니다.

- 역량 (Capability): LLM 제품이 의도한 기능을 제대로 수행하는가?
- 위험 (Risk): 제품의 결과물이 사용자나 비즈니스에 해를 끼칠 가능성은 없는가?

물론, 무엇이 '좋은' 결과물이고 무엇이 '잘못된' 결과물인지를 판단하는 구체적인 품질 기준은 제품의 사용 사례에 따라 천차만별입니다.

예를 들어, 고객 문의에 답변하는 질의응답(Q&A) 시스템을 개발한다면, 다음과 같은 세부 항목들을 평가해 볼 수 있습니다.

#### 역량 (Capability) 평가
- 정확성 (Correctness): LLM이 사실을 왜곡하지 않고, 사실에 기반한 정확한 답변을 제공하는가?
- 유용성 (Helpfulness): 답변이 사용자의 질문 의도를 파악하여 명확하고 충분하게 문제를 해결해 주는가?
- 어조 및 스타일 (Tone & Style): 답변의 톤이 명확하고 전문적이며, 우리 브랜드가 추구하는 스타일과 일치하는가?
- 형식 (Format): 답변이 정해진 길이 제한을 준수하고, 필요한 경우 출처 링크를 포함하는 등 요구되는 형식을 따르는가?

#### 위험 (Risk) 평가
- 환각 (Hallucinations): 사실과 다르거나 맥락에 없는 정보를 만들어내어 오해의 소지가 있는 답변을 제공하지는 않는가?
- 편향성 (Bias): 학습 데이터에 내재된 편견을 그대로 반영하여 특정 성별, 인종, 지역 등에 대해 편향된 응답을 하지는 않는가?
- 유해성 (Toxicity): 공격적이거나 모욕적인, 또는 사회적으로 부적절한 결과물을 생성할 가능성은 없는가?
- 민감 정보 유출 (Sensitive Data Leakage): 사용자의 악의적인 질문(Prompt Injection 등)에 속아 학습 데이터에 포함된 개인정보나 기업의 기밀 데이터를 외부에 노출하지는 않는가?

효과적인 LLM 평가를 설계하려면, 우리가 만든 {{<hl-text primary>}}제품의 명확한 목적, 잠재적 위험, 그리고 테스트 과정에서 주로 관찰되는 오류 유형{{</hl-text>}}을 기반으로 평가 기준의 범위를 좁혀야 합니다. 평가는 그저 점수를 매기는 행위가 아니라, '새로운 프롬프트가 더 나은가?' 혹은 '지금 제품을 출시해도 괜찮은가?'와 같은 <strong>실질적인 의사결정</strong>으로 이어져야 합니다.

예를 들어, '유창성(fluency)'이나 '일관성(coherence)' 같은 일반적인 기준을 생각해 봅시다. 최신 LLM들은 대부분 이미 인간처럼 자연스럽고 논리적인 글을 쓰는 데 탁월합니다. 이런 모델을 대상으로 유창성 점수를 완벽하게 받는 것은 서류상으로는 보기 좋을지 몰라도, 제품 개선에 아무런 도움이 되지 않는 '의미 없는 지표'일 뿐입니다. (물론, 성능이 낮은 소규모 언어 모델을 사용한다면 유창성 테스트가 의미 있을 수도 있습니다.)

때로는 긍정적으로 여겨지는 기준조차 절대적이지 않습니다. 대부분의 경우 '사실적 정확성'은 매우 중요한 가치입니다. 하지만 만약 우리가 만드는 제품이 <strong>마케팅 캠페인 아이디어를 생성하는 도구</strong>라면 어떨까요? {{<hl-text primary>}}이때는 사실보다 오히려 기발한 상상력이 더 핵심적인 가치가 됩니다. 이런 상황에서는 '환각(Hallucination)'을 무조건적인 오류로 보기보다, '창의성'과 '다양성'의 원천으로 보고 더 높게 평가해야 할 수도 있습니다.{{</hl-text>}} 

이것이 바로 LLM 제품 평가와 LLM 벤치마크의 근본적인 차이점입니다. 이것이 바로 LLM 제품 평가와 LLM 벤치마크의 근본적인 차이점입니다. 벤치마크가 광범위한 기술을 측정하는 <strong>학교 시험</strong>이라면, LLM 제품 평가는 우리 시스템이 '채용'된 특정 역할을 얼마나 뛰어하게 수행하는지 확인하는 <strong>직무 성과 평가</strong>에 가깝다고 할 수 있습니다.

---
## LLM 평가의 어려움
이렇듯이 LLM 기반 제품의 평가는 간단한 과정이 아닙니다. 품질을 측정하는 기준이 제품에 따른 맞춤형일 뿐만 아니라, 평가 방식 자체가 기존의 소프트웨어 테스트나 예측 머신러닝의 접근법과 근본적으로 다르기 때문입니다.

#### LLM의 비결정성 (Non-determinism)
LLM은 확률적으로 결과물을 생성합니다. 이는 동일한 질문이나 명령(입력)에 대해서도 매번 다른 결과가 나올 수 있음을 의미합니다.

이러한 특성은 창의적이고 다채로운 답변을 가능하게 하는 장점이 되지만, 평가의 일관성을 확보하기 어렵게 만드는 요인이 됩니다. 생성된 다양한 결과물들이 모두 우리가 설정한 <strong>정답</strong>의 기준을 충족하는지 검증해야 하므로 테스트가 훨씬 복잡해집니다.

#### 정해진 정답의 부재 (Absence of a Single Correct Answer)
기존 머신러닝 시스템, 예를 들어 스팸 분류기는 이메일이 <strong>스팸</strong>인지 <strong>아닌지</strong>처럼 미리 정의된 정답 중 하나를 예측합니다. 따라서 정답 레이블이 있는 데이터셋을 이용해 모델이 얼마나 정답을 잘 맞히는지 쉽게 측정할 수 있습니다.

하지만 LLM은 주로 이메일 초안을 작성하거나 사용자와 대화하는 것처럼 정답이 무한히 많을 수 있는 <strong>'열린 과제(Open-ended Task)'</strong>를 수행합니다. 좋은 이메일을 작성하는 방법은 셀 수 없이 많기 때문에, 하나의 모범 답안과 단순히 비교하는 방식으로는 성능을 제대로 평가할 수 없습니다. 대신, 우리는 의미의 유사성이나 답변의 스타일, 어조, 안전성 같은 훨씬 주관적이고 복합적인 특징을 평가해야만 합니다.

#### 예측 불가능한 입력과 실제 환경의 변수
LLM 제품은 매우 광범위한 사용자 시나리오를 처리해야 합니다. 예를 들어, 고객 지원 챗봇은 제품 스펙, 반품 절차, 계정 문제 해결 등 수많은 종류의 문의에 대응해야 합니다. 예상되는 모든 입력을 포괄하는 테스트 데이터셋을 구축하는 것 자체가 또 하나의 거대한 도전 과제입니다.

더 큰 문제는, 우리가 아무리 철저하게 테스트를 하더라도 <strong>실제 사용자는 늘 우리의 예상을 뛰어넘는다는 것</strong>입니다. 실제 사용자들이 던지는 예측 불가능한 입력은 테스트 환경에서는 발견하지 못했던 새로운 오류나 결과를 초래할 수 있습니다. 이 때문에 제품 출시 이후에도 실시간으로 서비스 품질을 관찰하고 평가하는 온라인 모니터링 시스템이 반드시 필요합니다.

---
## 정리
지금까지 LLM 제품이란 무엇인지 알아보고, LLM 모델 평가와 LLM 제품 평가가 어떻게 다른지, 그리고 제품 평가가 어려운 이유는 무엇인지에 대해 다루었습니다. 좋은 제품을 만들기 위해서는 모델의 일반적인 성능 점수뿐만 아니라, 특정 사용 사례에 맞는 맞춤형 평가가 필수적임을 확인했습니다.

LLM 평가의 기초를 다진 이 글에 이어, 다음 글에서는 실제 평가를 위한 신뢰할 수 있는 데이터셋을 구축하는 과정을 구체적인 사례를 통해 살펴보겠습니다.

---
#### Reference
Evidently AI | [LLM evaluation: a beginner's guide](https://www.evidentlyai.com/llm-guide/llm-evaluation)

Hendrycks, Dan (2020) | [Measuring Massive Multitask Language Understanding
](https://arxiv.org/abs/2009.03300)
---