---
title: DeepSeek OCR (2/2)
date: 2025-10-23
categories:
- LLM
thumbnailImage: /images/LLM/4/1.webp
summary: DeepSeek OCR 모델 아키텍처 리뷰
katex: true
---
## Model Architecture
DeepSeek OCR의 아키텍처는 기존 VLM들과 동일하게 비전 인코더와 LLM으로 구성됩니다.

비전 인코더는 사전 학습된 모델의 성능을 최대한 활용하고자 SAM-base와 CLIP-large를 직렬로 연결하여 사용하고 있고, LLM은 DeepSeek-3B(Active parameter 570M)을 사용하고 있습니다.

{{<image classes="fancybox center" src="/images/LLM/4/1.webp">}}
> 그림 1. Model Architecture (DeepSeek OCR)<br>
[DeepSeek-OCR: Contexts Optical Compression](https://github.com/deepseek-ai/DeepSeek-OCR)

비전 인코더의 출력을 LLM에 주입(Injection)하는 여러 방식 중, 최근에는 토큰 임베딩 단계에서 텍스트와 이미지 임베딩 토큰을 이어붙이거나 삽입하는 방식을 주로 택하고 있습니다.

#### 비전 인코더
DeepSeek OCR의 비전 인코더에서 SAM-base 모델은 $n$개의 16x16 패치로 분할된 이미지를 입력으로 받고, CLIP-large 모델은 SAM-base 모델의 출력 결과를 16배 다운샘플링하여 입력으로 사용합니다.

```python
self.patch_embed = PatchEmbed(...)  # 쪼개진 Patch들을 임베딩하는 레이어

self.pos_embed = nn.Parameter(...)  # Position Embedding (like bias)

self.blocks = nn.ModuleList()       # Transformer Blocks
for i in range(depth):
    block = Block(...)
    self.blocks.append(block)

self.neck = nn.Sequential(          # Neck 레이어
    nn.Conv2d(kernel_size=1, ...),  # 피처맵의 채널 수를 조절하고 (768 → 256)
    LayerNorm2d(out_chans),
    nn.Conv2d(kernel_size=3, ...),  # 3x3 Conv를 통해 지역적인 특징을 한 번 더 정제
    LayerNorm2d(out_chans),
)

# 각각 채널 수를 2배로 늘리고, 가로 세로 크기를 2배 줄인다. (x4 x4 = x16)
self.net_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False)
self.net_3 = nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1, bias=False)
```

```python
class CLIPVisionEmbeddings(nn.Module):
    ...
    self.class_embedding = torch.nn.Parameter(torch.randn(self.embed_dim))
    ...
    def forward(self, pixel_values, patch_embeds):
        ...
        class_embeds = self.class_embedding.expand(batch_size, 1, -1)
        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)
```

SAM-base 모델의 경우, Neck 및 다운샘플링 레이어를 제외하면 기존 ViT와 구조적으로 큰 차이가 없습니다. CLIP-large 모델 또한 기존 ViT와 유사하지만, 언어 모델의 `[CLS]` 토큰에 해당하는 `class_embedding`을 삽입하는 과정이 추가된 점이 다릅니다.

---
## Image Processing
DeepSeek OCR의 이미지 처리 프로세스는 <strong>기본 해상도(Native resolution)</strong>와 <strong>동적 해상도(Dynamic resolution)</strong>의 2가지 모드를 지원합니다.

기본 해상도는 Tiny, Small, Base, Large의 네 가지 하위 모드를 지원하며, 각각에 해당하는 해상도와 토큰 수는 512×512 (64), 640×640 (100), 1024×1024 (256), 1280×1280 (400)입니다. {{<hl-text primary>}}Tiny와 Small 모드{{</hl-text>}}는 해상도가 비교적 작기 때문에 비전 토큰 낭비를 피하고자 원본 이미지의 형태를 직접 {{<hl-text primary>}}<strong>리사이징</strong>{{</hl-text>}}하여 처리합니다. {{<hl-text primary>}}Base와 Large 모드{{</hl-text>}}의 경우, 원본 이미지의 종횡비를 보존하기 위해 이미지를 해당 크기에 맞게 {{<hl-text primary>}}<strong>패딩(padding)</strong>{{</hl-text>}}합니다.

동적 해상도는 두 개의 기본 해상도를 조합하여 구성합니다. 예를 들어, <strong>건담(Gundam) 모드</strong>는 $n$개의 640×640 <strong>타일(local views, 지역 시점)</strong>과 1개의 1024×1024 <strong>타일(global view, 전역 시점)</strong>로 구성되어 있습니다. 타일링 방식은 [InternVL2.0](https://internvl.github.io/blog/2024-07-02-InternVL-2.0/)의 전략을 따르며, 기본적으로 패딩을 활용하여 각 타일의 영역이 겹치지 않도록 처리합니다. {{<hl-text primary>}}4k 같은 초고해상도 이미지의 경우, 타일 수가 일정 개수를 초과하지 않도록 종횡비를 유지한 채 이미지 전체를 다운샘플링한 후 패딩을 적용합니다.{{</hl-text>}}

이러한 동적 해상도를 지원하는 주된 이유는 신문과 같이 초고해상도 이미지를 효과적으로 처리하기 위함입니다. 타일링 방식은 활성화 메모리 사용량을 줄이는 데 효과적이며, 기본 타일 해상도가 비교적 크기 때문에 이미지가 지나치게 잘게 조각나지 않습니다.

건담 모드는 네 가지 기본 해상도 모드와 함께 학습하지만, 건담-마스터(Gundam-master) 모드(1024×1024 지역 시점 + 1280×1280 전역 시점)는 이미 학습된 DeepSeek OCR 모델에 추가 학습을 통해 구현됩니다. 이는 건담-마스터의 해상도가 매우 커서 함께 학습시킬 경우 전체 학습 속도를 저하 시킬 수 있으므로, <strong>부하 분산(load balancing)을 고려한</strong> 결정입니다.

#### 예제
> 768x1024 이미지 (Base 모델):<br>
1024x1024에 맞추기 위해, 부족한 가로 영역(768→1024)을 패딩합니다.

> 600x1200 이미지 (Large 모델):<br>
1280x1280에 맞추기 위해, 가로(600→1280)와 세로(1200→1280) 영역을 모두 패딩합니다.

> 1920x1000 이미지 (건담 모드):<br>
(지역 시점) 640x640 타일 크기에 맞춰 세로 영역을 1280으로 패딩한 후, 가로 3개, 세로 2개의 총 6개 타일로 분할합니다.<br>
(전역 시점) 원본 이미지를 1024x1024 크기로 리사이징하여 전체 문맥을 파악하는 데 사용합니다.

> 4096x3072 이미지 (건담-마스터 모드):<br>
(지역 시점) 1024x1024 타일로 나누면 4x3=12개로, 9개를 초과하므로, 가장 긴 쪽(가로, 4개 타일)을 3개 타일에 맞추기 위해, 종횡비를 유지하며 이미지 전체를 3072x2304 크기로 리사이징합니다.<br>
이후 부족한 세로 영역을 3072로 패딩하여 3072x3072 크기의 캔버스를 만듭니다. 최종적으로 가로 3개, 세로 3개, 총 9개의 타일로 분할합니다.<br>
(전역 시점) 원본 이미지를 1280x1280 크기로 리사이징합니다.

---
## 결론
많은 기술을 숙지하는 것도 어렵지만, 그것을 한곳에 유기적으로 녹여내는 것은 차원이 다른 문제입니다. 이 모델은 그 어려운 조합을 성공적으로 구현해낸 것 같습니다. 기존 모델의 사전 학습 파라미터를 활용하면 학습 시간을 단축할 수 있지만, 말처럼 간단한 작업은 아닙니다. 특히, 각기 다른 목적으로 학습된 세 개의 모델을 효과적으로 이어붙인 점이 매우 인상적입니다.

이미지 처리 부분에서 기본 해상도와 동적 해상도의 개념 자체는 다른 연구에서 가져왔다고 볼 수도 있습니다. 하지만 이 모델은 적절한 압축 크기를 결정하고, 초고해상도 처리는 부하 분산을 고려하여 별도 모드로 추가 학습시키는 등, 구현 과정에서 많은 엔지니어링 노하우를 엿볼 수 있습니다.

다만 아쉬운 점은, 논문이 너무 많은 외부 레퍼런스에 의존하고 있다는 것입니다. 논문 본문만으로는 모델의 정확한 동작 방식을 완전히 파악하기 어려워, 세부 사항을 이해하기 위해 원본 레퍼런스나 코드를 직접 살펴봐야 했기에 파악하는데 오랜 시간이 걸렸습니다.

---
#### Reference
DeepSeek-AI | [deepseek-ai/DeepSeek-OCR](https://huggingface.co/deepseek-ai/DeepSeek-OCR)

Chen, Zhe (2024) | [InternVL2: Better than the Best—Expanding Performance Boundaries of Open-Source Multimodal Models with the Progressive Scaling Strategy](https://internvl.github.io/blog/2024-07-02-InternVL-2.0/)

Kirillov, Alexander (2023) | [Segment Anything](https://ai.meta.com/research/publications/segment-anything/)

Radford, Alec (2021) | [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)

---