---
title: Gated Delta Networks
date: 2025-09-19
categories:
- LLM
thumbnailImage: /images/LLM/2/1.png
summary: Qwen3-Next 모델의 핵심인 Gated Delta Networks에 대한 설명
katex: true
---
2025년 9월, Qwen3-Next 모델이 공개되었습니다. 새롭게 공개된 Qwen3-Next-80B-A3B 모델은 기존  Qwen3-32B 모델보다 10.7배 빠른 추론 속도를 기록했으며, 각종 벤치마크에서도 더 뛰어난 성능을 입증했습니다.

가장 주목할 만한 점은 10배 이상 향상된 추론 속도입니다. 이는 <strong>Gated Delta Networks</strong>라는 새로운 선형 트랜스포머(Linear Transformer) 구조를 채택한 덕분입니다. 이 구조는 특히 긴 컨텍스트(long context)를 처리할 때 뛰어난 성능 개선을 보여주며 기존 언어 모델들의 한계를 넘어섰습니다.

{{<image classes="fig-80 center" src="/images/LLM/2/1.png">}}
> 그림 1. Hybrid Architecture (Qwen3-Next)<br>
[QwenTeam (2025)](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list)

그림 1과 같이 Qwen3-Next 모델은 Gated DeltaNet과 Gated Attention을 결합한 하이브리드 구조를 채택했으며, 그림 1의 'Output Gate'에 해당하는 output gating 메커니즘을 활용하여 어텐션(Attention)의 low-rank 이슈를 개선하는 등 다양한 기술적 시도를 담아냈습니다.

또한, 80B의 파라미터 중 3.7%에 해당하는 약 3B의 파라미터만 사용하는 Ultra-Sparse MoE를 성공적으로 학습하여 효율성을 극대화했습니다.

마지막으로, 다중 토큰 예측(Multi-Token Prediction, MTP)을 도입하여, 기존 모델이 매 스텝(step)마다 하나의 토큰을 순차적으로 생성했던 것과 달리, {{<hl-text primary>}}Qwen3-Next는 스텝마다 여러 개의 토큰을 예측할 수 있게 되었습니다.{{</hl-text>}}

---
## Mamba2
Gated Delta Network는 Delta Network에 Mamba2의 Gating 메커니즘을 더한 구조입니다. 이름에서 알 수 있듯 Mamba2의 기반이 되는 Mamba1 모델도 있지만, Mamba2를 이해하는 데 필수적인 내용은 아니므로 설명은 생략하겠습니다.

Mamba2는 기존 트랜스포머의 주요 병목이었던 <strong>이차 복잡도(quadratic complexity, $O(L^2)$)</strong> 문제를 <strong>선형 복잡도(linear complexity, $O(L)$)</strong>로 개선하기 위해 제안된 모델입니다. 핵심 아이디어는 시퀀스(Sequence) 데이터를 처리할 때 어텐션(Attention)을 사용하는 대신, <strong>상태(State)</strong> 개념을 도입하여 순차적으로 정보를 누적하는 것입니다. 이는 Residual Connection과 유사한 방식입니다.

$$
\tag{1} S_t = \alpha_t S_{t-1} + v_t k_t^\intercal \in \R^{d_v \times d_k}
$$

$$
\tag{2} o_t = S_t q_t = \sum^t_{i=1} (v_i k_i^\intercal) q_t = \sum^t_{i=1} v_i (k_i^\intercal q_t) \in \R^{d_v}
$$

$$
\tag{3} O = (QK^\intercal \odot M) V \in \R^{L \times d_v}
$$
위 수식을 통해 Mamba2의 작동 방식을 살펴보겠습니다.

<strong>수식 (1)</strong>은 상태($S$)가 업데이트되는 과정을 보여줍니다. 현재 타임스텝($t$)의 상태($S_t$)는 이전 상태($S_{t-1}$)에 <strong>감쇠율(decay rate, $\alpha_t$)</strong>을 곱해 과거 정보의 영향력을 조절하고, 현재 토큰의 Key-Value 벡터($v_t k_t^\intercal$)를 더하여 계산됩니다. 이처럼 이전 정보를 선별적으로 유지하며 다음 상태로 넘겨주는 순환적인 구조를 가집니다.

<strong>수식 (2)</strong>에서는 이렇게 업데이트된 상태($S_t$)와 현재 토큰의 쿼리($q_t$)를 곱해 최종 출력($o_t$)을 계산합니다. 최종적으로는 Query, Key, Value가 모두 계산에 사용되므로, <strong>수식 (3)</strong>으로 표현되는 표준 트랜스포머의 어텐션 메커니즘과 유사한 역할을 수행합니다. {{<hl-text primary>}}그렇다면 연산량의 차이는 어디서 발생할까요?{{</hl-text>}}

핵심은 상태($S_t$)의 차원에 있습니다. Mamba2의 상태($S_t$)는 시퀀스 길이($L$)와 무관하게 $d_v \times d_k$라는 고정된 크기를 가집니다. 따라서 각 토큰을 처리하는 연산은 시퀀스가 길어져도 일정하게 유지되며, 전체 시퀀스를 처리하는 데 필요한 총연산량은 시퀀스 길이에 선형적으로 비례($O(L)$)합니다.

반면, 표준 트랜스포머는 <strong>수식 (3)</strong>에서 볼 수 있듯, 시퀀스 내의 모든 토큰 쌍에 대한 유사도($Q K^\intercal$)를 계산해야 합니다. 이로 인해 시퀀스 길이가 길어질수록 연산량이 제곱($L \times L$)으로 늘어나는 이차 복잡도($O(L^2)$) 문제가 발생합니다. 바로 이 지점이 Mamba2가 트랜스포머 대비 긴 시퀀스에서 압도적인 효율성을 보이는 이유입니다.

---
## Delta Network
Mamba2의 상태(State) 업데이트는 이전 상태에 감쇠율($\alpha_t$)을 곱하는 비교적 단순한 방식이었습니다. 반면 <strong>Delta Network</strong>는 <strong>Delta Update Rule</strong>을 통해 이전 상태($S_{t−1}$)에서 {{<hl-text primary>}}불필요한 정보는 지우고 새로운 정보는 선별적으로 쓰는 동적인 메모리 업데이트 메커니즘을 사용합니다.{{</hl-text>}}

$$
\tag{4} S_t = S_{t-1} - \underbrace{(S_{t-1} k_t)k_t^\intercal}_ {v_t^{old}} + \underbrace{(\beta_t v_t + (1 - \beta_t) S_{t-1} k_t)) k_t^\intercal}_ {v_t^{new}} = S_{t-1} (I - \beta_t k_t k_t^\intercal) + \beta_t v_t k_t^\intercal
$$

<strong>수식 (4)</strong>를 살펴보면, 새로운 상태($S_t$)는 이전 상태($S_{t-1}$)에서 {{<hl-text primary>}}잊어야 할 과거 정보($v_t^{old}$)를 빼고, 새롭게 쓸 정보($v_t^{new}$)를 더하는{{</hl-text>}} 직관적인 형태로 계산됩니다. 이 수식을 정리한 가장 오른쪽 항을 보면 Delta Network의 핵심 원리를 파악할 수 있습니다. 바로 ($I - \beta_t k_t k_t^\intercal$) 항으로 <strong>하우스홀더 변환(Householder transformation)</strong> 행렬에서 영감을 받은 형태입니다.

본래의 하우스홀더 행렬은 벡터를 특정 축에 대해 반사(reflection)시키는 직교 행렬(orthogonal matrix)로, 벡터의 길이를 보존하는 성질이 있습니다. 델타넷은 하우스홀더 변환 행렬과 유사한 Key 벡터의 외적 항($k_t k_t^\intercal$)을 포함하고 있지만, 직교성을 보존하지 않는다는 결정적인 차이가 있습니다. 이는 곧 벡터의 길이를 보존하지 않는다는 의미이며, 덕분에 델타넷은 단순히 정보를 반사하는 것을 넘어, 정보의 중요도에 따라 선택적으로 기억을 강화하거나 약화시키는 유연한 변환을 수행할 수 있게 됩니다.

---
## Gated Delta Network
Gated Delta Network는 이름에서 알 수 있듯, Delta Network의 구조에 Mamba2의 Gating 메커니즘을 추가한 형태입니다.

$$
\tag{5} S_t = S_{t-1} (\alpha_t (I - \beta_t k_t k_t^\intercal)) + \beta_t v_t k_t^\intercal
$$

수식 (5)는 앞서 살펴본 Delta Network의 수식 (4)에서 이전 상태($S_{t-1}$)에 감쇠율($\alpha_t$)을 곱해주는 항이 추가된 구조입니다. 여기서 한 가지 의문이 생길 수 있습니다. Delta Network는 정보를 <strong>선택적으로</strong> 지우기 위해 고안되었는데, 왜 전체 정보를 일괄적으로 잊게 하는 게이팅 메커니즘을 추가한 것일까요? 이는 언뜻 보기에 역설적입니다. {{<hl-text primary>}}이러한 의문은 Delta Network의 핵심 역할을 어떻게 바라보느냐에 따라 해소됩니다.{{</hl-text>}}

Delta Network의 하우스홀더 변환 기반 업데이트는 단순히 정보를 지우거나 더하는 행위가 아닙니다. 만약 아무런 변형 없이 이전 상태($S_{t−1}$)의 정보를 계속해서 누적하기만 한다면, 학습 과정에서 기울기(gradient)가 소실되거나 폭발할 위험이 커집니다.

즉, {{<hl-text primary>}}Delta Network는 이러한 기울기 문제를 텐서의 변형을 통해 해결하면서 이전 상태의 정보를 다음 상태로 안정적으로 전달하는 것이 주된 목적입니다.{{</hl-text>}} 그러다 보니 정보 '보존'에는 탁월한 능력을 보이지만, 역설적으로 정보를 '잊는'데는 비효율적일 수 있습니다. 기본적으로 보존에 초점이 맞춰져 있기 때문입니다.

따라서 여기에 Mamba2의 게이팅 메커니즘($\alpha_t$)을 추가하여, 필요할 때 과거 정보를 효율적으로 잊을 수 있는 능력을 보완해 주는 것입니다. 결과적으로 Gated Delta Network는 정보를 안정적으로 보존하는 동시에 효과적으로 잊는, 두 가지 능력을 모두 갖춘 균형 잡힌 구조가 됩니다.

---
## Chunkwise parallel form
Chunkwise Parallelism이란, 이름 그대로 기존에 단일 상태(state) 단위로 순차 처리하던 작업을 청크(chunk) 단위로 묶어 병렬로 처리하는 기법입니다. 기존의 선형 어텐션이나 RNN 계열 모델에서 상태 ($S_t$)는 다음과 같이 계산됩니다.

$$
S_t = S_{t-1} + \Delta S_t
$$

여기서 문제는 $S_t$ 를 계산하려면 반드시 $S_{t-1}$ 의 계산이 완료되어야 한다는 <strong>순차적 의존성</strong>입니다. 이러한 방식은 GPU의 가장 큰 장점인 병렬 연산 능력을 거의 활용하지 못합니다. 상태 변화량($\Delta S_1, \Delta S_2, \dots, \Delta S_n$) 자체는 모든 토큰에 대해 한 번의 병렬 연산으로 처리할 수 있지만, 이를 누적하는 과정($\Delta S_1 = S_0 + \Delta S_1, S_2 = S_1 + \Delta S_2, \dots, S_n = S_{n-1} + \Delta S_n$)은 GPU의 수많은 코어 중 단 하나만을 사용하여 순서대로 처리해야 합니다.

<br>
{{<hl-text primary>}}이 문제를 벽돌 1,000개를 쌓는 작업에 비유해 보겠습니다.{{</hl-text>}} 기존 방식은 한 명의 작업자가 벽돌 1,000개를 순서대로 쌓습니다. Chunkwise 방식은 10명의 작업자가 각자 100개씩 벽돌 더미를 동시에 쌓고, 마지막에 그 10개의 더미를 순서대로 합칩니다.

당연히 10명이 협력하는 후자의 방식이 훨씬 빠릅니다. Chunkwise Parallelism은 이와 같이 병렬 처리 구간을 최대한 늘리고, 순차 처리 구간을 최소화하는 전략입니다.

이를 똑같이 선형 트랜스포머에 적용하면, {{<hl-text primary>}}기존 $O(N)$의 복잡도가 $O(N/C + logC)$로 개선됩니다. N이 1,000이고 C가 100이면 50배 빨라지는 것 입니다.{{</hl-text>}}

---
## Multi Token Prediction
<strong>다중 토큰 예측(Multi-Token Prediction, MTP)</strong>이라는 개념을 처음 접하면, 여러 토큰을 동시에 추론하는 것은 조합의 경우의 수가 기하급수적으로 늘어나기 때문에 수학적으로 비효율적이고 구현이 불가능하다고 생각할 수 있습니다.

실제로 MTP는 여러 토큰을 한 번에 생성하는 것이 아니라, 작고 빠른 초안(draft) 모델이 예측을 생성하면 크고 정확한 검증(verify) 모델이 그 결과를 확인하는 방식으로 동작합니다. 이 과정은 <strong>추측성 디코딩(Speculative Decoding)</strong>이라고도 불립니다.

- <strong>초안 모델</strong>이 "오늘 날씨가"라는 문장 뒤에 ("아주", "좋을", "것", "같아요")라는 예측을 빠르게 생성합니다.
- <strong>검증 모델</strong>이 ("아주", "좋을", "것", "같아요")가 모두 맞다고 확인하면, 후보 시퀀스 전체를 채택합니다.
- <strong>검종 모델</strong>이 ("아주", "맑을", ...)을 예측한다면,  "맑을" 까지만 채택하고 다시 초안 모델을 사용하여 예측을 생성합니다.

이 방법이 효율적인 이유는 생성(Generation)과 검증(Verification)의 속도 차이에 있습니다. 일반적인 모델은 다음 토큰을 예측하기 위해 순차적으로 연산을 반복해야 하지만, 이미 만들어진 문장이 정답인지 '검증'하는 작업은 단 한 번의 연산으로 병렬 처리할 수 있기 때문입니다. 결과적으로 여러 토큰을 순차적으로 생성하는 것보다 훨씬 빠른 속도를 달성할 수 있습니다.

---
## 결론
Songlim Yang 연구자의 기여(contribution)를 바탕으로 Qwen팀이 유의미한 성과를 만들어내면서, 대형 언어 모델(LLM) 분야에 새로운 패러다임이 열리고 있는 것 같습니다. 덕분에 모델 학습에 필요한 막대한 비용이 줄어들었고, 추론 속도와 메모리 사용량 또한 획기적으로 개선되었습니다.

개인적으로 이 새로운 구조가 기존 트랜스포머보다 훨씬 합리적(reasonable)이라고 느끼는 이유는, 기존 Residual Network과 동일한 철학을 공유하기 때문입니다.

기존의 Scaled Dot-Product Attention은 성능 자체는 뛰어났지만, 어텐션 스코어를 시각화해보면 우리가 직관적으로 예상하는 모습과는 거리가 멀 때가 많았습니다. 또한, 자기회귀(Autoregressive) 생성을 위해 미래 정보를 가리는 마스킹(Masking) 기법 역시 다소 인위적인 제약처럼 느껴졌습니다.

하지만 선형 트랜스포머는 이러한 문제들로부터 비교적 자유롭습니다. {{<hl-text primary>}}상태(state)를 순차적으로 업데이트하는 방식은 훨씬 직관적이고 자연스러운 모델 구조로 보입니다.{{</hl-text>}} 이러한 이유로, 저는 앞으로 대부분의 LLM이 선형 트랜스포머 구조를 기반으로 발전할 것이라고 조심스럽게 예측해 봅니다.

---
#### Reference

Yang, Songlim (2024) | [Parallelizing Linear Transformers with the Delta Rule over Sequence Length](https://arxiv.org/pdf/2406.06484)

Yang, Songlim (2024) | [GATED DELTA NETWORKS: IMPROVING MAMBA2 WITH DELTA RULE](https://arxiv.org/abs/2412.06464)

Qwen Team (2025) | [Qwen3-Next: Towards Ultimate Training & Inference Efficiency](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list)

---