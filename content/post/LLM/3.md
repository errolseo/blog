---
title: DeepSeek OCR (1/2)
date: 2025-10-22
categories:
- LLM
thumbnailImage: /images/LLM/3/1.webp
summary: DeepSeek OCR 논문 리뷰
katex: true
---
## Vision Language Model
최근 멀티모달(Multimodal) LLM에 대한 관심이 증가하고 있습니다. 기존의 LLM이 주로 텍스트 데이터 처리에 중점을 둔 반면, 멀티모달 LLM은 텍스트, 이미지, 음성 등 여러 형식의 데이터를 통합하여 더 복잡하고 다재다능한 작업을 수행하는 확장된 개념입니다. 그 중에서도 {{<hl-text primary>}}이미지와 텍스트를 함께 처리하는 Image-Text-to-Text(IT2T) 모델이 최근 빠르게 성장하고 있습니다.{{</hl-text>}}

IT2T 모델 또는 Vision Language Model(VLM)은 기존 LLM(Qwen, Gemma 등)에 비전 인코더(Vision Encoder) 모듈을 추가한 구조입니다. 이 모델은 <strong>텍스트만 입력될 때는 기존 LLM과 동일하게 동작</strong>하지만, <strong>이미지가 함께 입력되면</strong> 비전 인코더가 이미지를 임베딩한 후 그 결과를 텍스트 임베딩 사이에 삽입하여 LLM의 최종 입력으로 사용합니다.

이러한 VLM의 활용 방안은 여러 가지가 있지만, 우선 `gemma-3`, `QWEN3-VL`처럼 기존 LLM에 이미지 입력 기능만 추가하여 동일한 방식으로 학습시키는 것입니다. 이런 VLM은 기존 LLM과 사용법은 완전히 같으면서, 이미지를 추가로 처리할 수 있다는 장점을 가집니다.

---
## VLM OCR
VLM 모델은 기본적으로 OCR이 가능합니다. 이미지와 함께 '텍스트를 추출해 줘'와 같은 프롬프트를 입력하면 이미지 속 텍스트를 추출할 수 있습니다. 하지만 범용 VLM의 OCR 성능에는 한계가 있으며, 이를 극복하기 위해서는 `PaddleOCR-VL`이나 `rednote-hilab/dots.ocr`처럼 문서 파싱(Documents Parsing)에 특화된 대규모 OCR 데이터로 지시 학습(Instruction learning)을 추가 진행해야 합니다.

추가 학습을 거친 모델은 OCR 및 Layout Detection 부분에서 기존 범용 VLM보다 뛰어나지만, 반대로 다른 언어 이해 능력은 `gemma-3`나 `QWEN3-VL`에 비해 떨어지는 경향을 보이게 됩니다.

글의 주제인 <strong>DeepSeek OCR</strong>의 경우도 표면적으로는 문서 파싱 전문 모델에 가까워 보입니다. 학습 데이터로 OCR과 CLIP 데이터셋만을 사용하였고, 성능 평가 또한 문서 OCR 벤치마크인 OmniDocBench를 활용했기 때문입니다. 하지만 논문에서 다루는 내용을 살펴보면 흥미로운 지점이 발견됩니다. {{<hl-text primary>}}이들의 연구 목표는 단순히 OCR 성능 향상이 아니라, <strong>'텍스트 토큰의 효율적인 압축 방안'</strong>에 있다는 것입니다.{{</hl-text>}}

예를 들어, 문서를 OCR로 스캔해 텍스트로 변환하면 토큰 수가 1,000~2,000개에 달할 수 있지만, 문서를 이미지로 입력받아 비전 인코더로 압축하면 단 100여 개의 토큰으로 표현할 수 있습니다. 이는 입력 토큰을 획기적으로 줄이는 압축 효과를 가져옵니다. 이처럼 DeepSeek 팀의 최종 목표는 '어떻게 비전 인코더와 언어 모델을 효과적으로 End-to-End로 연결할 것인가'라는 점에서 gemma-3나 Qwen3-VL과 같은 범용 VLM의 지향점과 맞닿아 있습니다.

이러한 연구 방향성은 논문의 결론(Conclusion) 부분에 명확하게 드러납니다.

> 이 기술 보고서에서 저희는 DeepSeek-OCR을 제안하고 이 모델을 통해 <strong>컨텍스트의 광학적 압축(contexts optical compression)</strong>에 대한 실현 가능성을 예비적으로 검증했습니다. {{<hl-text primary>}}이를 통해 모델이 소수의 비전 토큰으로부터 그 수량의 10배를 초과하는 텍스트 토큰을 효과적으로 디코딩할 수 있음을 보여주었습니다.{{</hl-text>}} 저희는 이 발견이 향후 VLM과 LLM의 발전을 촉진할 것이라고 믿습니다.
<br>
또한, DeepSeek-OCR은 대규모 사전 학습 데이터 생산이 가능한 매우 실용적인 모델로서, LLM에게 없어서는 안 될 보조 역할을 할 수 있습니다. 물론, OCR만으로는 진정한 컨텍스트 광학 압축을 완전히 검증하기에 충분하지 않으며, 저희는 향후 digital-optical text interleaved 사전 학습, needle-in-a-haystack 테스트 및 기타 평가들을 수행할 것입니다. <strong>(6. Conclusion 번역)</strong>

결국 DeepSeek-OCR은 단순히 뛰어난 OCR 모델을 넘어, LLM의 긴 컨텍스트 처리 문제를 '이미지를 통한 텍스트 압축'이라는 새로운 관점으로 해결하려는 새로운 시도라고 볼 수 있습니다.

---
## Contribution
컨텍스트의 광학적 압축(contexts optical compression) 등의 관점은 흥미롭습니다만, 현실적인 논문의 가장 큰 기여점은 기존 VLM보다 빨라졌다는 점입니다.

{{<image classes="fig-80 center" src="/images/LLM/3/1.webp">}}
> 그림 1. Performance on Omnidocbench<br>
[DeepSeek-OCR: Contexts Optical Compression](https://github.com/deepseek-ai/DeepSeek-OCR)

트랜스포머(Transformer) 모델의 어텐션(Attention) 구조 상, 입력의 제곱에 비례해서 연산 복잡도가 늘어나기 때문에, 입력 토큰 수의 감소는 추론 속도에 큰 영향을 줄 수 밖에 없습니다. {{<hl-text primary>}}놀라운 점은 비전 토큰 압축을 통해 추론 속도가 향상되었는데, 제공된 벤츠마크 상으로는 성능의 트레이드 오프가 전혀 없다는 점 입니다.{{</hl-text>}}

벤치마크 상으로 기존 SOTA 모델은 `rednote-hilab/dots.ocr`(200dpi)의 0.125(Edit Distance)입니다. 하지만 DeepSeek OCR은 dots.ocr보다 33%(1853/5545) 적은 비전 토큰을 활용해서 0.123(Edit Distance)를 달성했습니다.

추가적으로 논문은 비전 토큰을 얼마나 압축할 수 있는가에 대한 지표를 표를 이용해 제시합니다.

{{<image classes="fig-80 center" src="/images/LLM/3/2.webp">}}
> 표 1. Performance based on vision-text compression ratio<br>
[DeepSeek-OCR: Contexts Optical Compression](https://github.com/deepseek-ai/DeepSeek-OCR)

기본적으로 압축률이 높아질수록 Precision이 하락하는 음의 상관관계(Negative Correlation)을 보여줍니다. 여기에 토큰 수라는 변수를 추가하여, 토큰 수에 따라 최적의 압축률이 상이할 수 있다는 점을 보여줍니다.

---
## 결론
`rednote-hilab/dots.ocr` 모델이 처음 공개되었을 때, 허깅페이스 커뮤니티는 뜨겁게 달아올랐습니다. Layout Detection부터 OCR까지 End-to-End 방식으로 한 번에 처리하면서도, 그 성능이 놀라웠기 때문입니다. DeepSeek OCR의 성능 자체는 dots.ocr과 크게 차이 나지 않기 때문에 놀라운 부분은 아닙니다. 속도가 빨라진 점도 충분히 예상 가능한 발전입니다. dots.ocr만큼 놀랍지는 않았습니다.

그런데 DeepSeek 팀이 제시한 패러다임은 충분히 흥미롭습니다. 제 개인적인 해석으로는, DeepSeek 팀은 이제 '문서를 굳이 텍스트로 변환할 필요가 없다' 는 메시지를 제시하고 있는 것 같습니다. 문서는 이미지 그 자체로 충분한 의미와 구조를 담고 있으니, 기존의 '파싱(Parsing)&rsquo;이라는 개념 자체가 불필요해질 수 있다는 것 입니다. 논문에서 보여준 결과를 보면 이 주장은 상당한 설득력을 가집니다.

현재 많은 기업이 문서 파싱을 통해 RAG 시스템을 구축하고, 더 나아가 LLM과 MCP를 이용해 Agentic AI를 도입하려 하고 있습니다. 만약 DeepSeek 팀의 주장처럼 문서 처리의 근본적인 패러다임이 바뀐다면, 우리는 지금까지와는 완전히 다른 시스템 아키텍처를 고민해야 할지도 모릅니다.

---
#### Reference
DeepSeek-AI | [deepseek-ai/DeepSeek-OCR](https://huggingface.co/deepseek-ai/DeepSeek-OCR)

Xiaohongshu | [rednote-hilab/dots.ocr](https://huggingface.co/rednote-hilab/dots.ocr)

---