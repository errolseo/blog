---
title: "[OCR] Synthetic Text Image (1)"
date: 2022-03-08
categories:
- OCR
tags:
- Synthetic Text Image
- MJSynth
- SynthText
thumbnailImage: /images/OCR/2-1.png
summary: MJSynth, SynthText
katex: true
---
{{< hl-text primary >}}딥 러닝 프로젝트은 <b>데이터</b>와 <b>모델</b>의 확보가 시작이자 끝이다.{{< /hl-text >}} 그 중 데이터에 대해 다룰 것인데, 데이터 중에서도 OCR의 **인조 데이터**에 대해서 다룰 것이다. 만약 독자가 OCR을 위해 대량의 scene text image를 annotation할 자금이나 인력을 보유하고 있다면 이 글은 별로 도움이 되지 않을 수도 있다. 하지만 그렇지 않다면 이 글은 독자에게 필수적인 내용이 될 것이다.

#### \#
[MJSynth(MJ)](https://www.robots.ox.ac.uk/~vgg/publications/2014/Jaderberg14c/)는 word box image를 생성하는 이미지 합성 엔진이자 그것을 이용해서 만든 데이터셋을 말한다. MJ는 비교적 간단한 방법들을 이용하여 **대량의 데이터**를 생성해냈다. 데이터 생성 파이프라인은 Font rendering, Border/shadow rendering, Base coloring, Projective distortion, Natural data blending, Noise의 총 6단계로 논문에 소개되어 있다. 각 모듈들의 사용처는 직관적이지만 자세한 메커니즘은 꽤 복잡할 수도 있다.

{{< image classes="center" src="/images/OCR/2-1.png" title="Figure 1. MJSynth pipeline" >}}

1\. Font rendering
- [Google Fotns](https://fonts.google.com/)에서 다운로드한 1,400개의 폰트
- 자간(kerning), 두께(weight), 밑줄(underline) 등의 속성
- 사전 정의된 90k words의 dictionary $\mathcal{W}$
- Horizontal bottom, random curve 등의 형태 변형

2\. Border/shadow rendering
- Inset/outset border, shadow 등

3\. Base coloring
- 외부 데이터에서 추출한 3색 1쌍의 color map
- 글자, 배경 및 기타 여러 요소들의 색상을 정하는데 사용

4\. Projective distortion

{{< image classes="center" src="/images/OCR/2-2.png" title="Figure 2. Projective transformation" >}}

5\. Natural data blending
- ICDAR 2003 and SVT 등의 실제 이미지와 합성 [(Blend mode wikipedia)](https://en.wikipedia.org/wiki/Blend_modes)

6\. Noise
- Gaussian noise, blur, jpeg 손실 압축 왜곡 등의 노이즈

2014년에 만들어진 MJ 데이터는 scene text image는 아니기에 별도로 학습된 scene text detection(STD) 모델이 필요하다는 치명적인 단점이 존재한다. 하지만 scene text recognition(STR) 모델의 성능에 크게 기여하였고, 그 기본적인 방향성은 후에 이루어진 많은 연구에 영향을 주었다.

#### \#
[SynthText(ST)](https://www.robots.ox.ac.uk/~vgg/publications/2016/Gupta16/)의 기본적인 pipeline은 MJ와 유사하다. 다만 추가적인 기술을 이용해 양적인 측면과 질적인 측면 모두 양호한 scene text image를 생성해 냈다. ST는 MJ와 함께 많은 논문에서 pre-trained 모델을 학습하는데 사용되고 있다.

{{< image classes="center" src="/images/OCR/2-3.png" title="Figure 3. Poisson Image Editing (JM Di Martino et al.)" >}}
{{< image classes="center" src="/images/OCR/2-4.png" title="Figure 4. Contour Detection and Hierarchical Image Segmentation (Pablo Arbelaez et al.)" >}}

표본집단 $\mathrm{n}$(인조 데이터)의 이상적인 목표는 모집단 $\mathrm{N}$과 동일한 수준의 **다양성**과 **유사성**을 확보하는 것이다. 왜냐하면 모집단에 존재할 수 없는 데이터를 포함하고 있거나, 한 쪽으로 편향된 학습 데이터는 모델의 성능에 악영향을 주기 때문이다.

ST는 이런 문제점을 해결하기 위해 [poisson image editing](https://www.cs.jhu.edu/~misha/Fall07/Papers/Perez03.pdf)을 이용해 다양한 배경 위에 텍스트가 자연스럽게 녹아들 수 있도록 했고, [gPb-UCM](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/papers/amfm_pami2010.pdf)을 통해 얻은 이미지의 경계선을 이용해 텍스트의 위치가 자연스러워지도록 제어했다. 필자가 ST에서 가장 주목하는 부분은 gPb-UCM 등의 boundary detection이다.

필자는 인조 데이터 생성에서 가장 피해야할 부분이 모집단에 존재할 수 없는 데이터를 생성하는 것이라 생각한다. 다양성과 유사성이 중요하다고 말하고 있지만, 둘 중 더 중요한 것을 뽑으라고 한다면 단언컨대 다양성이다. {{< hl-text primary >}}시험 범위 안의 내용을 완전히 이해했다면 만점을 받을 것이며, 시험 범위 밖의 내용을 추가적으로 학습하는 것은 결과에 영향을 주지 않는다.{{< /hl-text >}} 하지만 모집단과 유사한 수준의 다양성을 **생성**해내는 것은 불가능에 가깝다. 그렇다면  모집단의 데이터와 최대한 유사해져야 한다.

{{< image classes="center" src="/images/OCR/2-5.png" title="Figure 5. (좌) ST 예시 이미지 / (우) 현실에선 존재할 수 없는 텍스트" >}}

ST의 예시에서 놀란 부분은 정말 현실에 있을 것 같다는 점이다. 반대로 우측의 필자가 합성한 이미지는 홀로그램을 사용한 것이 아닌 이상 존재하기 어려운 형태이다. 그럼 이러한 유사성은 모델에 어떤 도움이 되는 것인가, 예시를 통해 알아보자

#### \#
{{< image classes="center" src="/images/OCR/2-6.png" title="Figure 6. (좌) 타밀 문자 로고 / (우) 한글 로고" >}}
{{< image classes="center" src="/images/OCR/2-7.png" title="Figure 7. 타밀어 스타벅스 간판" >}}

타밀 문자를 접해본 적이 없다면 Fig.6의 타밀 문자 로고를 보고 글자라고 생각하기 어려울 수 있다. 반대로 한글을 접해본 적 없는 외국인이라면 한국어 로고를 보고 단순한 그림이라고 생각하기 쉽다. 하지만 우리는 처음보는 언어라 하더라도 Fig.7의 간판을 보고 **식별이 불가능하더라도** 높은 확률로 글자일 것이라 예측할 수 있다. 연속성을 가진 무언가가 글자가 위치할 수 있는 곳에 존재하기 때문이다. {{< hl-text primary >}}이러한 <b>일반화(generalization)</b>된 추상적인 지식이 바로 데이터의 유사성이 중요한 이유다.{{< /hl-text >}}

우리는 **글자가 위치할 수 있는 곳**에 대해 따로 학습하거나 배운 적이 없어도 경험에 의해 어렴풋이 그 개념을 추상적으로 이해하고 있다. 이 능력이 우리를 배우지 않은 시험 범위의 문제도 얼추 맞출 수 있게 해준다. 그 말인즉 다양성의 부족을 유사성이 어느 정도 메꿔줄 수 있다는 것이다.

#### \#
데이터 수집의 경우 데이터의 편향이 있을지언정 그 데이터는 모집단 내의 데이터가 확실하다. 하지만 데이터 생성의 경우 모집단에선 존재할 수 없는 형태의 데이터를 생성할 가능성이 존재한다. Fig.5의 우측 이미지처럼 일반적이지 않은 데이터를 계속해서 학습하면 모델은 결국 글자가 위치할 수 있는 곳, 더 나아가서는 텍스트에 대한 추상화에 실패할 것이다.

#### Experience
> Open Source : https://github.com/ankush-me/SynthText

```dockerfile
# Dockerfile
FROM nvcr.io/nvidia/pytorch:21.12-py3

ENV LD_PRELOAD=/opt/conda/lib/libmkl_core.so:/opt/conda/lib/libmkl_sequential.so

RUN apt-get update && apt-get install -y libsm6 libxext6 libxrender-dev && rm -rf /var/lib/apt/lists/*
RUN pip install h5py pygame
RUN git clone -b python3 https://github.com/ankush-me/SynthText.git
```

- fonts
- color map
- 배경 및 배경의 boundary 정보
- ...

ST를 이용한 데이터 생성에는 필요한 준비물들이 많다. 그 밖에도 ST로 데이터를 생성하는데 여러 애로사항에 부딪힐 수 있는데, 그 모든 것을 다루는 것은 글의 취지가 아니기 때문에 위의 개발 환경만 공유한다.

결론만 말하면 당연하게도 위의 오픈 소스와 개발 환경을 이용해 데이터를 생성할 수 있다. 하지만 Fig.5의 예시 이미지 수준의 퀄리티를 기대하지는 말자. 실제 ST의 데이터셋도 그렇고 필자가 직접 생성해 본 결과 Fig.5와 유사한 수준의 결과가 나오기도 하지만, 그 확률이 낮다. ST는 2016년에 발표된 이미지 합성 엔진으로 개선 되지 않은 6년 전 버전을 그대로 사용하는 것은 아쉬운 부분이 많다.

#### Reference
[1] M. Jaderberg, et al. [Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition](https://www.robots.ox.ac.uk/~vgg/publications/2014/Jaderberg14c)

[2] A. Gupta, et al. [Synthetic Data for Text Localisation in Natural Images](https://www.robots.ox.ac.uk/~vgg/publications/2016/Gupta16)

[3] P. Perez, et al. [Poisson Image Editing](https://www.cs.jhu.edu/~misha/Fall07/Papers/Perez03.pdf)

[4] P. Arbelaez, et al. [Contour Detection and Hierarchical Image Segmentation](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/papers/amfm_pami2010.pdf)
