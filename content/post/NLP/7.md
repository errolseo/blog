---
title: Training Data (1/2)
date: 2099-12-31
categories:
- NLP
thumbnailImage: 
summary: ""
---
2019년에 공개되었던 <strong>GPT-2</strong>는 137M 모델로 300B 토큰을 학습했습니다. 이는 현재 H100 1장을 1시간 빌리는데 $1.47이므로(vast.ai) 약 $100가 있으면 재현할 수 있습니다. 하이퍼파라미터 최적화에 10회 정도의 학습이 필요하다면 약 $1,000로 한화로 130-150만원이 있으면 재현 가능합니다.

하지만 최근 공개된 <strong>Gemma 3</strong>의 경우, 27B 모델로 14T 토큰을 학습하였고 이는 약 $938,000이고 비슷하게 최적화에 10회 정도의 학습이 필요하다면 재현하는데 한화로 약 130억원이 필요합니다. 게다가 H100 1,000장 가량을 가지고 1회 학습에 약 26일이 걸리기 때문에 재현하는데 시간 비용도 260일이 필요합니다. 더 큰 문제는 위의 내용은 이상적인 수치일 뿐이며 H100 1,000장을 한번에 학습시킬 시스템을 확보하는 것 자체가 일반 회사에서는 거의 불가능에 가깝다는 것입니다. (게다가 14T 토큰은 또 어떻게 확보하고... 실제로 학습했다고 해도 그 때 되면 또 기술격차가 벌어져서 사실상 260일 + 130억 써서 만든 모델이 경제적 가치가 사라져버리는 일이 발생)

---
#### Reference

---