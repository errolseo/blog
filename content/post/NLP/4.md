---
title: "Vector Similarity Search"
date: 2025-06-13
categories:
- NLP
tags:
- Natural Language Processing
- Language Model
- Retrieval-Augmented Generation
thumbnailImage: /images/NLP/4-1.jpg
summary: Introduce about vector similarity search + HNSW
showActions: True
---
<strong>벡터 유사도 검색(Vector Similarity Search)</strong>은 데이터를 숫자의 배열인 벡터로 표현하고, 이 벡터들 간의 유사도를 측정하여 가장 유사한 데이터를 찾아내는 방법입니다. 예를 들어, 'Cat'이라는 단어는 'Dog'와 'Kitten' 중에서 'Kitten'과 더 비슷한 맥락에서 사용될 것입니다. 또 다른 예시로, 'Dog'와 'Wolf' 중에서는 'Dog'의 쓰임새가 'Cat'과 더 유사하다고 볼 수 있습니다.

{{< image classes="fig-60 center" style="width: 400px;" src="/images/NLP/4-1.jpg" title="Figure 1. Word Embeddings" >}}

여기서 <strong>'쓰임새가 유사하다'</strong>는 것은 단어의 <strong>의미(semantic)</strong>가 유사하다고 표현할 수 있습니다. 이러한 의미 정보를 벡터로 표현하는 것을 <strong>임베딩(Embedding)</strong>이라고 하며, 의미가 유사할수록 벡터 간의 거리가 가깝게 표현되도록 임베딩 모델을 학습합니다. 이 과정은 단어뿐만 아니라 문장 단위에서도 동일하게 적용될 수 있으며, {{< hl-text primary >}}임베딩과 벡터 유사도 검색을 통해 <strong>RAG(Retrieval-Augmented Generation)</strong>에서 문서를 효과적으로 검색할 수 있게 됩니다.{{< /hl-text >}} 

---

## Vector Similarity
{{< image classes="fig-75 center" style="width: 400px;" src="/images/NLP/4-2.jpg" title="Figure 2. Vector Similarity" >}}

|Distance-based|Angle-based|
|:---:|:---:|
|직관적|비직관적|
|크기 정보 유지|크기 정보 손실|
|정규화 어려움|정규화된 데이터|
|고차원에서 변별력 약화 (차원의 저주)|차원의 저주에 덜 민감|

\
벡터의 유사도를 측정하는 방법은 크게 거리 기반(distance-base) 측정과 각도 기반(angle-based) 측정 두 가지로 나뉜다. 

---

## kNN (k-Nearest Neighbors)

---

## HNSW (Hierarchical Navigable Small World)

HNSW는 **AkNN(Approximate K-Nearest Neighbor) 알고리즘**의 한 종류입니다. 이는 가장 가까운 이웃을 '근사적으로' 찾는 방식이기 때문에, 항상 전역 최적해(global minima)를 보장하지는 않습니다.

HNSW의 가장 중요한 특징은 **계층 구조**입니다. 상위 레이어는 하위 레이어의 **진입점(entry point)**을 결정하는 역할을 합니다. 이게 무슨 의미인지 좀 더 자세히 설명해보겠습니다.

#### HNSW의 검색 원리

우리가 가진 모든 벡터 데이터를 [그림 1]의 Layer 1처럼 그래프의 노드로 표현했다고 가정해 봅시다. 그리고 상위 레이어들은 모든 노드를 가지고 있지 않고, 일부 노드만 보유하게 합니다. 이러한 계층 구조로 Layer를 복제하는 이유는 앞서 말했듯이, **하위 레이어의 진입점을 효율적으로 찾기 위해서**입니다.

기존 kNN 알고리즘은 모든 점들과 쿼리 간의 거리를 비교하여 최근점접을 찾거나, 클러스터링을 통해 일정 영역의 점들과 거리를 비교하는 방식을 취했습니다. 하지만 HNSW의 경우, 그래프 탐색을 수행하여 쿼리와 가장 가까운 점들을 확보하게 됩니다. 그럴 경우 가장 중요한 것은, 탐색을 시작하게 될 그래프의 진입점(entry point)입니다. entry 포인트를 랜덤으로 설정할 경우 최악의 경우는 모든 점들을 탐색해야하기 때문입니다.

그렇기 때문에 계층 구조로 Layer를 복제하여 상위 레이어에서 빠르게 최근접 노드를 찾고, 하위 레이어에서 그 노드를 진입점으로 이용하는 전략을 채택하는 것 입니다.

#### HNSW의 주요 하이퍼파라미터

- **M (Max Connection)**
각 노드가 HNSW 그래프의 각 레이어에서 가질 수 있는 최대 양방향 연결(엣지)의 수
- **efConstruction**
HNSW 인덱스를 구축하거나 새로운 노드를 삽입할 때, 각 노드가 자신의 이웃을 찾기 위해 탐색하는 **후보 이웃 노드의 최대 수**
    - 이 값은 얼마나 "꼼꼼하게" 주변을 탐색하여 최적의 연결을 만들지를 결정합니다.
- **ef (efSearch)**
실제 쿼리의 최근접 이웃을 탐색할 때, 현재까지 찾은 최적의 후보 노드 목록(dynamic candidate list)의 최대 크기
    - 높은 `ef` : 검색 정확도(Recall) 향상, 검색 시간(Latency) 증가
    - 낮은 `ef` : 검색 정확도 저하, 검색 시간 단축
- **mL (Level Multiplier)**
각 노드가 더 높은 레이어에 포함될 확률, 일반적으로 1/ln(M) 값을 사용
    - `mL` 값이 클수록, 상위 레이어에 더 많은 노드가 존재하게 됩니다.
    이는 계층 탐색의 이점을 줄여 진입점을 찾는 시간을 증가시킬 수 있습니다.
    - 반대로 `mL` 값이 너무 작으면, 상위 레이어에 노드가 너무 적어 최적의 진입점을 찾기 어려워 탐색 시간이 증가할 수 있습니다.

요약하자면, `M`과 `efConstruction`에 따라 최하위 레이어에서 그래프가 생성되고, `mL`에 의해 선별된 노드들은 상위 레이어로 올라가 동일한 과정을 반복합니다. 이렇게 만들어진 계층적 그래프를 통해 효율적으로 벡터를 탐색하게 되는 것입니다.

#### HNSW에 대한 TMI

HNSW에는 한 가지 까다로운 문제점이 존재합니다. 삽입(insert)은 비교적 간단하지만, 삭제(delete)의 경우 기존 방법론들과 달리 큰 문제가 될 수 있기 때문입니다. 만약 노드가 제거될 때 연결을 재정의하지 않으면 특정 노드는 의도치 않게 모든 연결이 끊기는 등의 그래프의 무결성이 손상될 수 있기 때문입니다.

이러한 문제를 해결하기 위해 HNSW에서는 일반적으로 그래프에서 노드를 즉시 제거하기보다는 비활성화하는 방식을 채택하고 있습니다. 삭제된 노드들을 "삭제된 노드”로 표시한 후 커넥션은 유지합니다. 그리고 일정 주기마다 정리(vacuuming)하거나 재구성(rebalancing)하는 **lazy deletion** 방식을 사용합니다.

HNSW는 초기에 그래프를 구축하는 데 비교적 많은 비용이 들지만, 검색 속도와 정확도 면에서는 높은 성능을 보장합니다. 그렇기 때문에 HNSW는 현존하는 AkNN 방법론 중 가장 효율적이고 널리 사용되는 알고리즘 중 하나입니다. 물론 같은 HNSW라도 메모리 최적화 방법이나, 다양한 트레이드오프를 통해 개선된 버전들이 계속 연구되고 있습니다.

## 결론

---

#### Reference
[A Gentle Introduction to Vector Databases](https://weaviate.io/blog/what-is-a-vector-database) | Leonie Monigatti, Zain Hasan
