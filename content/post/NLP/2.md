---
title: "[DL] Representation"
date: 2022-06-07T00:00:00+09:00
categories:
- DL
tags:
- Deep Learning
- Representation
thumbnailImage: /images/DL/1-2.png
summary: Introduction to Language Model
katex: true
---
인공 지능(Artificial Intelligence)에 대한 정의는 꽤나 심오하다. 인공 지능까지 가지 않아도 지능이란 것을 정의하는 것은 꽤나 심오하다.

현재 AI 분야에서 대부분의 엔지니어 및 연구자들의 업무는 딥 러닝을 이용해서 인간의 업무를 대체하거나 보조하는 신경망 모델(Neural Network Model)을 개발하는 것이다. "신경망 모델은 인공 지능인가?" 에 대한 논의는 넘어가기로 하고, 신경망 모델 자체에 대한 정의를 해보자. 신경망 모델은 하나의 함수(Function)다. 입력을 넣으면 출력을 받을 수 있다.

{{< image classes="center" src="/images/DL/1-1.webp" title="Figure 1. Black box function" >}}

이러한 신경망 모델은 입력과 출력 그리고 모델 자체도 중요하지만, 딥 러닝이 이렇게까지 성공할 수 있었던 이유는 **hidden representation**에 있다. Embedding, Feature, Representation 상황에 따라 쓰이는 용어는 다르지만 이들에게는 공통점이 존재한다. **알려주지 않은 부분**을 스스로 학습한다는 것이다.

#### \#

자세한 내용을 이야기하기 전에 그림을 한 장 먼저 보도록 하자.

{{< image classes="center" src="/images/DL/1-2.png" title="Figure 2. T-SNE visualization of image representation mapped semantic word space" >}}

Zero-Shot Learning Through Cross-Modal Transfer, R. Socher et al.의 저자들은 Wikipedia text를 이용해 사전 학습된 word embedding 모델의 representation을 이용해서 image 데이터의 zero-shot learning 모델을 학습시키려는 시도를 했다.  

$$\begin{equation}
J(\Theta) = \sum_{y\in Y_{s}}\sum_{x^{(i)}\in X_{y}} \Big\lVert w_{y} - \theta^{(2)} f(\theta^{(1)} x^{(i)})\Big\rVert^{2},
\end{equation}$$

All training images $x^{(i)} \in X_{y}$ of a seen class $y \in Y_{s}$ are mapped to the word vector $w_{y}$.  
$\Theta = (\theta^{(1)}, \theta^{(2)})$ : network weights. ($f =$ tanh)

\
Zero-shot learning 모델의 objective function은 위와 같이 단순하다. Fig. 2에서 word box는 word embedding 모델의 representation을 시각화한 것이고, 각 문양들은 학습 image와 unseen image를 zero-shot learning 모델에 입력으로 넣었을 때의 representation을 시각화한 것이다. 여기서 unseen image는 cat과 truck이다. 이제 생각해봐야 할 일은 **알려주지 않았지만 모델이 스스로 학습한 부분**이다.

ㅇ

#### \#



#### Reference
[1]  R. Socher et al. [Zero-Shot Learning Through Cross-Modal Transfer](https://nlp.stanford.edu/~socherr/SocherGanjooManningNg_NIPS2013.pdf)

[2] A []()

[3] A []()
