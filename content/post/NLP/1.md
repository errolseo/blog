---
title: "[NLP] Natural Language Processing"
date: 2022-05-24T00:00:00+09:00
categories:
- NLP
tags:
- Natural Language Processing
thumbnailImage: /images/NLP/1-1.png
summary: Introduction to NLP
katex: true
---
>미 버클리 대학생 리암 포어(Liam Porr)가 인공지능을 사용해 작성한 블로그 게시물이 IT뉴스 큐레이팅 플랫폼인, 해커뉴스(Hacker News)에서 1위를 차지했다.\
...\
해커뉴스 기사들은 각 기사당 눌린 '좋아요' 수와 게시된 날짜를 기준으로 순위를 매긴다.\
\
출처 : Ai 타임스 (http://www.aitimes.com/news/articleView.html?idxno=131593)

#### \#
National Language Processing(NLP). 자연어 처리는 근래 눈부신 속도로 발전했다. 그 중심에는 [Transformer(Attention Is All You Need)](https://arxiv.org/abs/1706.03762)가 있지만, Transformer 이전에도 word2vec, RNN 등을 이용해 이미 가파른 성장세를 보여주고 있었다. 이러한 성장은 rule base였던 자연어 처리 분야가 word2vec을 시작으로 model base로 점점 전환되면서 시작되었다.

Rule base 자체를 부정하는 것은 아니지만 한계점이 존재하는 것은 명백한 것은 확실하다. 반면 model base에는 명확한 한계점이 존재하지 않는다. Rule base로 뉴스 플랫폼에서 1위를 할 수 있는 기사를 작성할 수는 없다. Rule base로 법률을 본래 목적을 이해하여 개인의 사정에 의거한 판결을 내릴 수는 없다. Rule base로 사람과 다양한 주제에 대해 자연스럽게 대화할 수는 없다. 하지만 놀랍게도 model base로는 위에 나열된 모든 것들이 가능할지도 모른다.

그리고 model base가 됐다고 rule base 자체가 사리지는 것은 아니다. Rule base의 훌륭한 기술들은 model base의 전처리, 후처리 그 외의 다양한 부분에서 성능 향상에 기여를 하고 있다.

#### \#
그럼 rule base와 model base의 차이는 무엇인가. 필자는 'semantic에 대한 이해'라고 생각한다. word2vec을 예시로 이용해 알아보자.

{{< image classes="center" src="/images/NLP/1-1.png" title="Fig. 1: Word2vec example" >}}

Word2vec이란 단어를 다차원 벡터로 변환시켜주는 neural network model을 말한다. 여기서 중요한 점은 변환된 벡터가 단어가 가진 **의미론적** 정보를 포함하고 있다는 것이다.

$$\mathrm{w_\mathcal{king}} - \mathrm{w_\mathcal{man}} + \mathrm{w_\mathcal{woman}}  \approx \mathrm{w_\mathcal{queen}}$$

Word2vec 이전에는 king - man + woman = queen, 사람에게는 단순해 보이는 이 넌센스를 기계한테 학습시킬 방법이 놀랍게도 존재하지 않았다. Word2vec의 등장으로 다양한 이슈가 발생했는데, 

은결이는 <u>배</u>를 타고 강을 건넜다.
종길이는 과수원에서 <u>배</u>를 땄다.
창석이는 밥을 많이 먹어서 <u>배</u>가 부르다.

#### Experience


#### Reference
[1] 