---
title: "[NLP] Natural Language Processing"
date: 2022-05-16T00:00:00+09:00
categories:
- NLP
tags:
- Natural Language Processing
thumbnailImage: /images/NLP/1-1.png
summary: Introduction to NLP
katex: true
---
>미 버클리 대학생 리암 포어(Liam Porr)가 인공지능을 사용해 작성한 블로그 게시물이 IT뉴스 큐레이팅 플랫폼인, 해커뉴스(Hacker News)에서 1위를 차지했다.\
...\
해커뉴스 기사들은 각 기사당 눌린 '좋아요' 수와 게시된 날짜를 기준으로 순위를 매긴다.\
\
출처 : Ai 타임스 (http://www.aitimes.com/news/articleView.html?idxno=131593)

#### \#
National Language Processing(NLP). 자연어 처리는 근래 눈부신 속도로 발전했다. 그 중심에는 [Transformer(Attention Is All You Need)](https://arxiv.org/abs/1706.03762)가 있지만, Transformer 이전에도 word2vec, RNN 등을 이용해 이미 가파른 성장세를 보여주고 있었다. 이러한 성장은 rule based model(규칙 기반 모델)이 주류였던 자연어 처리 분야가 word2vec을 시작으로 neural network based model(신경망 기반 모델)로 점점 변화하면서 시작됐다.

규칙 기반 자체를 부정하는 것은 아니지만 한계점이 존재하는 것은 명백한 것은 확실하다. 반면 신경방 기반에는 명확한 한계점이 존재하지 않는다. 규칙 기반으로 뉴스 플랫폼에서 1위를 할 수 있는 기사를 작성할 수는 없다. 규칙 기반으로 법률의 본래 목적을 이해하여 개인의 사정에 의거한 판결을 내릴 수는 없다. 규칙 기반으로 사람과 다양한 주제에 대해 자연스럽게 대화할 수는 없다. 하지만 놀랍게도 신경망 기반으로는 위에 나열된 모든 것들이 가능할지도 모른다.

그리고 신경망 기반의 모델을 사용한다고 규칙 기반 자체가 사리지는 것은 아니다. 규칙 기반의 훌륭한 기술들은 신경망 기반의 전처리, 후처리 그 외의 다양한 부분에서 성능 향상에 기여를 하고 있다.

#### \#
기존 규칙 기반 모델들은 의미론적 정보보다는 미리 정의된 사전에 의존하거나 통계적 패턴을 이용해서 문제를 해결했었다. 이러한 방식은 높은 정확도(accuracy)에 비해 낮은 recall(재현율)을 보여주며, 같은 모델을 다른 도메인에 적용하면 결과가 좋지 못했다. 규칙 기반 모델은 장점이 명확했지만 확장하는 것이 어려웠으며, 사람들이 실제로 사용하는 언어는 지속적으로 사용법이 바뀌기 때문에 실생활에 적용하는 것이 쉽지 않아, 주로 domain-specific task를 위주로 발전했다.

그럼 규칙 기반과 신경망 기반의 차이는 무엇인가. 필자는 'semantic에 대한 이해'라고 생각한다. Word2vec은 단어를 다차원 벡터로 변환시켜주는 Neural Network Language Model(NNLM)의 한 종류이다. 여기서 중요한 점은 변환된 벡터가 단어의 **의미론적** 정보를 포함하고 있다는 것이다.

{{< image classes="center" src="/images/NLP/1-1.png" title="Fig. 1: Semantic in word2vec" >}}



$$\mathrm{w_\mathcal{king}} - \mathrm{w_\mathcal{man}} + \mathrm{w_\mathcal{woman}}  \approx \mathrm{w_\mathcal{queen}}$$

Word2vec 이전에는 king - man + woman = queen, 사람에게는 단순해 보이는 이 넌센스를 기계한테 학습시킬 방법이 존재하지 않았다.

이런 규칙 기반 모델의 문제는 바로 과적합(overfitting)과 일반화(generalization)의 문제다. Word2vec은 smantic에 대한 이해를 통해 Neural Network Language Model(NNLM)의 일반화에 큰 기여를 했다. 

#### \#


> 은결이는 배를 타고 강을 건넜다.\
> 종길이는 과수원에서 배를 땄다.\
> 창석이는 밥을 많이 먹어서 배가 부르다.

#### Experience


#### Reference
[1] 