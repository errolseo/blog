---
title: "[NLP] Natural Language Processing"
date: 2022-05-24T00:00:00+09:00
categories:
- NLP
tags:
- Natural Language Processing
- Language Model
thumbnailImage: /images/NLP/1-1.png
summary: Introduction to NLP
katex: true
---
>미 버클리 대학생 리암 포어(Liam Porr)가 인공지능을 사용해 작성한 블로그 게시물이 IT뉴스 큐레이팅 플랫폼인, 해커뉴스(Hacker News)에서 1위를 차지했다.\
...\
해커뉴스 기사들은 각 기사당 눌린 '좋아요' 수와 게시된 날짜를 기준으로 순위를 매긴다.\
\
출처 : Ai 타임스 (http://www.aitimes.com/news/articleView.html?idxno=131593)

#### \#
National Language Processing(NLP). 자연어 처리는 근래 눈부신 속도로 발전했다. 그 중심에는 [Transformer(Attention Is All You Need)](https://arxiv.org/abs/1706.03762)가 있지만, Transformer 이전에도 word2vec, RNN 등을 이용해 이미 가파른 성장세를 보여주고 있었다. 이러한 성장은 rule based model(규칙 기반 모델)이 주류였던 자연어 처리 분야가 word2vec을 시작으로 neural network based model(신경망 기반 모델)로 점점 변화하면서 시작됐다.

규칙 기반 자체를 부정하는 것은 아니지만 한계점이 존재하는 것은 명백한 것은 확실하다. 규칙 기반으로 뉴스 플랫폼에서 1위를 할 수 있는 기사를 작성할 수는 없다. 규칙 기반으로 기존 판례를 뒤집는 판결을 내릴 수는 없다. 규칙 기반으로 사람과 다양한 주제에 대해 자연스럽게 대화할 수는 없다. 하지만 신경망 기반으로는 가능할지도 모른다.

그리고 신경망 기반의 모델을 사용한다고 규칙 기반 자체가 사리지는 것은 아니다. 규칙 기반의 훌륭한 기술들은 신경망 기반의 전처리, 후처리 그 외의 다양한 부분에서 성능 향상에 기여를 하고 있다.

#### \#
기존 규칙 기반 모델들은 미리 정의된 사전에 의존하거나 통계적 패턴을 이용해서 문제를 해결했었다. 이러한 방식은 높은 정확도(accuracy)에 비해 낮은 recall(재현율)을 보여주며, 같은 모델을 다른 도메인에 적용하면 결과가 좋지 못했다. 규칙 기반 모델은 장점이 명확했지만 확장하는 것이 어려웠으며, 사람들이 실제로 사용하는 언어는 지속적으로 사용법이 바뀌기 때문에 실생활에 적용하는 것이 쉽지 않았다. 그로 인해 규칙 기반 모델은 주로 domain-specific task를 위주로 발전했다.

Word2vec은 단어를 다차원 벡터로 변환시켜주는 Neural Network Language Model(NNLM)이다. 여기서 중요한 점은 변환된 벡터가 단어의 **smantic** 정보를 포함하고 있다는 것이다.

{{< image classes="center" src="/images/NLP/1-1.png" title="Figure 1. Semantic of word2vec" >}}

$$\mathrm{w_\mathcal{king}} - \mathrm{w_\mathcal{man}} + \mathrm{w_\mathcal{woman}}  \approx \mathrm{w_\mathcal{queen}}$$

king - man + woman = queen, 사람에게는 단순해 보이는 이 넌센스를 word2vec 이전의 모델들은 표현하지 못했다. 하지만 word2vec은 따로 학습하지 않아도 단어를 사용하는 패턴을 보고 스스로 학습하는데 성공했다. 이 사실은 word2vec 모델이 단어의 **일반적인** 의미를 스스로 학습할 수 있다는 것을 의미하며, 이는 word2vec 모델이 semantic을 이해할 수 있다고 봐도 무방하다.

Semantic을 이해하지 않고, 사전과 통계에만 의존해서 문제를 푸는 규칙 기반 모델은 과적합(overfitting)과 일반화(generalization) 관점에서 큰 문제점을 가질 수 밖에 없다. 결국은 사전과 데이터에 의존하며, 학습 데이터에 과적합될 수 밖에 없는 구조이기 때문이다.

#### \#
Word2vec의 등장으로 NLP 분야는 한걸음 더 나아갔지만, 자연어 처리에서는 semantic만큼 **sequence** 또한 마찬가지로 중요했다.

> 은결이는 **배**를 타고 강을 건넜다.\
> 종길이는 과수원에서 **배**를 땄다.\
> 창석이는 밥을 많이 먹어서 **배**가 부르다.

\
우리는 세 문장에서 '배'가 가진 의미가 모두 다르다는 것을 알 수 있다. 그 이유는 앞뒤 문맥을 통해 유추할 수 있기 때문이다. 즉 같은 단어라도 앞뒤에 어떤 단어가 오는지에 따라 그 의미가 달라지며, 이것을 자연어 처리에서 단어는 sequence에 의해 의미가 바뀐다고 표현한다.

아쉽게도 word2vec은 이러한 sequence까지는 표현하지 못한다. 위의 세 문장에서 '배'라는 단어를 word2vec의 입력으로 넣으면 똑같은 벡터로 치환된다. 그 안에는 '타는 배', '먹는 배', '신체의 일부분인 배' 세 가지 모두에 대해 어느 정도 그 의미를 포함하고 있지만, 이는 일반화 관점에서 문제점가 된다. 

NLP 연구자들은 이 sequence를 표현하기 위해 끊임없이 노력했고, RNN과 Transformer 등의 encoder의 등장으로 모델을 이용해 word vector에 sequence 정보를 담을 수 있게 되었다.

{{< image classes="center" src="/images/NLP/1-2.png" title="Figure 2. RNN language model" >}}

RNN은 시계열 데이터를 학습하기 위해 개발된 모델이다. 이전 sequence의 데이터가 다음 sequence의 output에 영향을 줄 수 있도록 설계되었다. Fig.2에서는 데이터가 정방향으로만 전이되지만 언어의 특성상 역방향도 중요하므로 NLP task에서는 보통 정방향과 역방향을 같이 학습할 수 있는 BiLSTM같은 모델이 주로 사용된다.

하지만 RNN에는 치명적인 단점이 하나 있다. Sequence가 길어지면 예전에 입력 받은 데이터가 점점 잊혀진다는 것이다. RNN은 애당초 시계열 데이터를 학습하기 위해 고안된 모델이므로, 오래 전의 입력보다 바로 전의 입력이 더 큰 영향을 줄 수 있도록 설계되어 있다. NLP 연구자들은 attention 등을 이용해 문제를 해결하기 위해 노력했고, 결국 transformer 모델의 등장으로 큰 변환점을 맞이하게 된다.

{{< image classes="center" src="/images/NLP/1-3.webp" title="Figure 3. Data flow in RNN and Transformer" >}}

{{< hl-text primary >}}RNN encoder에서의 sequence는 time이다.{{< /hl-text >}} RNN은 정방향과 역방향, 순서에 따라서 데이터를 참조한다. {{< hl-text primary >}}하지만 Transformer encoder에서의 sequence는 position이다.{{< /hl-text >}} Fig.3처럼 모든 position은 동시에 서로를 참조하기 때문에 순서보다는 구성이 중요하다. 이로 인해 transformer는 서로 멀리 떨어진 단어끼리도 단어의 의미를 결정하는데 연관성이 크다면 정보의 손실없이 word vector에 그 의미를 포함시킬 수 있게 되었다.

#### \#
현재 자연어 처리의 대세 역시 딥 러닝을 이용한 모델이다. 데이터와 모델이 물론 중요하다. 하지만 자연어 처리가 가진 **semantic, sequence**의 중요성에 대해 짚고 넘어갈 필요가 있다고 생각했다. 결국 자연어 처리에서 데이터와 모델은 semantic과 squence를 잘 이해시키는 방향으로 진화하고 있다는 것이 필자의 생각이다.

#### Reference
[1] T. Mikolov et al. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)

[2] Y. Liu et al [Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention](https://arxiv.org/abs/1605.09090)

[3] J. Devlin [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)