---
title: Model Context Protocol (1/3)
date: 2025-07-04
categories:
- MCP
thumbnailImage: /images/MCP/1/1.png
summary: MCP에 대한 설명 (Tool Calling 개념 설명 및 실습)
---
> 대화형 인공지능은 <strong>"오늘 날씨가 어때?" 라는 질문에</strong> 대답할 수 있을까요?
<br>
> 대규모 언어 모델(Large Language Model, LLM) <strong>단독으로는 불가능</strong>합니다.

오늘의 날씨를 알기 위해서는 다음과 같은 과정이 필요합니다.
1. 유저의 시간과 장소를 파악해야합니다.
2. 해당 정보를 토대로 외부의 날씨 정보 시스템에 접근하여 날씨 정보를 얻어야합니다.

대화형 인공지능은 유용하고 다양한 작업에서 사람의 일을 돕고 있지만 대화형 인공지능의 전신인 LLM은 학습된 데이터 외의 정보를 가지고 있지 않기 때문에 미래의 날씨도 현재 유저의 위치도 스스로는 파악할 수 없습니다. 이러한 문제들을 해결하기 위해서 Tool calling 개념이 등장했습니다.

---
## ReAct
Tool calling이란 개념 자체는 오래 전부터 존재했었지만, LLM과 결합해서 의미있는 결과를 처음 보여준 것은 ReAct: Synergizing Reasoning and Acting in Language Models (Shunyu Yao, et al. 2022)라고 생각됩니다.

{{<image classes="fig-100 center fancybox" style="width: 400px;" src="/images/MCP/1/1.png">}}
> 그림 1. Comparison of prompting methods
<br>
> [Shunyu Yao, et al. (2022)](https://arxiv.org/abs/2210.03629)

해당 논문은 기존에 추론(Reasoning) 모델들이 해결하지 못했던 문제들을 추론과 행동(Action)의 교차(interleaved) 방식을 이용해 해결하는 방법을 제시했습니다. {{<hl-text primary>}}ReAct에서는 'Thought -> Action -> Observation -> Thought -> Action -> Observation'의 순환 구조를 이용{{</hl-text>}}해 외부 세계에서 얻은 정보로 추론을 보완하고, 환각(hallucination)을 줄이며, 더 사실에 기반한 결정을 내릴 수 있게 했습니다. 이러한 ReAct의 등장 이후, 2023년 ChatGPT에 Function calling이 공식적으로 도입되었습니다.

Tool calling이 제대로 주목받기 시작한 건 2024년 11월 Anthropic에서 Model Context Protocol(MCP)를 공개하고 난 다음입니다. Anthropic에서는 MCP 공개와 함께 Tool calling 모델에 대한 충분한 지시 학습을 거친 Claude 3.7 Sonnet과 Cursor AI 같은 Coding Agent의 등장으로 인해 Tool calling이 실제로 유용하다는 것을 전세계의 모두가 확인하게 됩니다.

지금부터 Tool calling이 무엇이고 MCP는 무엇이며 어떤 식으로 동작하는지 차근차근 살펴보도록 하겠습니다.

---
## Tool calling
Tool calling이란 LLM이 외부 도구, API, 함수 등을 호출하고 상호작용할 수 있도록 하는 기능입니다. 이는 LLM의 능력을 단순히 텍스트 생성과 이해를 넘어 확장시켜, 실제 세상의 정보를 검색하거나, 작업을 수행하거나, 특정 기능을 사용하는 등 훨씬 더 복잡하고 유용한 작업을 수행할 수 있도록 만듭니다.
<br>

Tool calling을 하기 위해서는 먼저 Tool로 사용할 API 혹은 함수 등을 개발하고, LLM이 해당 Tool을 호출할 수 있게 연결해주는 작업이 필요합니다. 예시로 위에서 설명한 날씨 정보를 알려주는 LLM Agent를 만들어 보겠습니다.

##### Function
```python
import requests

def get_weather(latitude, longitude):
    response = requests.get(f"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m")
    data = response.json()
    return data['current']['temperature_2m']
```

##### Function description
```json
{
  "type": "function",
  "function": {
    "name": "get_weather",
    "description": "Get current temperature for provided coordinates in celsius.",
    "parameters": {
      "type": "object",
      "properties": {
        "latitude": {"type": "number"},
        "longitude": {"type": "number"}
      },
      "required": ["latitude", "longitude"],
      "additionalProperties": false
    },
    "strict": true
  }
}
```

날씨 정보를 알려주는 LLM Agent를 만들기 위해서는 날씨 정보를 불러올 수 있는 `get_weather`라는 함수를 정의하고, 함수의 이름 및 매개변수(arguments)를 포함한 전반적인 설명서를 작성해 LLM에게 알려줄 필요가 있습니다.
이러한 설명서의 경우, 기존에는 포맷이 다양했지만 MCP의 등장 이후 어느 정도 정형화된 포맷으로 통일되었습니다.

Tool calling의 초기에는 ReAct, ReWOO(Binfeng Xu, et al.)와 같은 다양한 전략이 존재했지만 최근에는 `tokenizer_config.json`의 `chat_template` 옵션에 Tool calling 사용할 때의 시스템 프롬프트가 모델 별로 정의되어 있으며, {{<hl-text primary>}}해당 프롬프트로 충분한 지시 학습(Instruction tuning)과 강화 학습(Reinforcement Learning from Human Feedback)을 거쳤기 때문에 입출력을 따로 제어해 줄 필요는 없습니다.{{</hl-text>}} 

##### Tool calling system prompt (Qwen3)
```txt
<|im_start|>system
# Tools

You may call one or more functions to assist with the user query.

You are provided with function signatures within <tools></tools> XML tags:
<tools>
{"type": "function", "function": {"name": "get_weather", "description": "Get current temperature for provided coordinates in celsius.", "parameters": {"type": "object", "properties": {"latitude": {"type": "number"}, "longitude": {"type": "number"}}, "required": ["latitude", "longitude"]}}}
</tools>

For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
<tool_call>
{"name": <function-name>, "arguments": <args-json-object>}
</tool_call><|im_end|>
```

미리 정의된 system prompt의 <tools> </tools> 사이에 Function description이 들어가게 됩니다.

##### User query and Tool calling
```txt
<|im_start|>user
서울 날씨가 어때?<|im_end|>
<|im_start|>assistant
<tool_call>
{"name": "get_weather", "arguments": {"latitude": 37.5665, "longitude": 126.978}}
</tool_call><|im_end|>
```

User query, 여기서는 "서울 날씨가 어때?" 를 입력하면 LLM이 판단하여 `get_weather` 함수를 부르기 위한 json 형태의 출력을 작성합니다.

##### Tool response and LLM context
```txt
<|im_start|>user
<tool_response>
"{"time": "2025-07-02T05:45", "interval": 900, "temperature_2m": 27.9, "wind_speed_10m": 5.3, "weather_code": 51}"
</tool_response><|im_end|>
<|im_start|>assistant
현재 서울은 가랑비가 내리고 있으며, 기온은 27.9°C입니다.
<|im_end|>
```

보통의 경우 LLM inference server에서 Tool을 실행시키지 않습니다. Tool은 <tool_call> 토큰으로 감싸진 출력을 전달 받았을 경우, client에서 실행을 하고 결과값(<tool_response>)을 다시 user query로 전달합니다. 마지막으로 LLM이 더 이상 부를 함수가 없다고 판단되면 자연어 context를 출력하고 Tool calling sequence가 마무리 됩니다.

---
### 마무리

Tool calling에서도 대화형 인공지능(LLM)은 <strong>{{<hl-text primary>}}자연어를 입력받아서, 자연어를 출력할 뿐{{</hl-text>}}</strong>입니다. 그 과정을 제어하고 외부 도구들과 연결하는 일은 인간의 역할입니다. <strong>아직까지는</strong> 말이죠.

---
#### Reference
[1] Yao, Shunyu (2022) | [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)

[2] OpenAI | [Function calling](https://platform.openai.com/docs/guides/function-calling?api-mode=chat)

---